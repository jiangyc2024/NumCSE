% The exercise requires:
% The solutions require:

\begin{samproblem}*{}{Arrow matrix-vector multiplication}[1](1){
    An ``arrow matrix'' $\VA$ is constructed given two vectors $\mathbf{a}$ and $\mathbf{d}$.
    The matrix is then squared and multiplied with a vector $\Vx$. This is implemented in the
    following C++ function:

\samincludecpp{../Codes/MatVec/ArrowMatrix/templates/arrowmatvec.cpp}[0]

}

%%%%%%%%%%%% SUBPROBLEM 1
\begin{subproblem}{}

  For general vectors $\mathbf{d} = (d_1, \dots, d_n)^\top$ and $\mathbf{a} = (a_1, \dots, a_n)^\top$
  sketch the matrix $\VA$ created in the function \verb|arrow_matrix_2_times_x|.

  \begin{samwriteprbpart}{sol}
  \begin{samsolution}
    The matrix $\mathbf{A}$ is the following:
    \begin{align}
    \VA=\begin{pmatrix}
      d_1 &     &        &         & a_1     \\
          & d_2 &        &         & a_2     \\
          &     & \ddots &         & \vdots  \\
          &     &        & d_{n-1} & a_{n-1} \\
      a_1 & a_2 & \dots  & a_{n-1} & d_n     \\
    \end{pmatrix}
    \end{align}
    The file \texttt{arrowmatvec.cpp} contains a detailed description of the code.
  \end{samsolution}
\end{samwriteprbpart}
\end{subproblem}

%%%%%%%%%%%% SUBPROBLEM 2
\begin{subproblem}{}
  We measure the run time of the function \verb|arrow_matrix_2_times_x| (written in the
  file \texttt{arrowmatvec.cpp})
  and plot the results in Figure \ref{fig:arrowmatvectiming}.

  Give a detailed explanation of the results.

 \begin{figure}[ht]
\centering
\includegraphics[width=0.8\textwidth]{../Codes/MatVec/ArrowMatrix/arrowmatvec_timing.eps}
\caption{timings for \texttt{arrowmatvec(d,a,x)}}
\label{fig:arrowmatvectiming}
\end{figure}

 \begin{samwriteprbpart}{sol}
  \begin{samsolution}
  The standard matrix-matrix multiplication has runtime growing with $O(n^3)$ and the standard
  matrix-vector multiplication has runtimes growing with $O(n^2)$. Hence, the overall
  computational complexity is dominated by $O(n^3)$.

  In this case, the line affecting the complexity is \verb|y = A*A*x|.

  Remember that most of C++ operators have a \textbf{left-to-right} precedence. This means that,
  even if,
  mathematically, $(\mathbf{A} \mathbf{A}) \mathbf{y} = \mathbf{A} (\mathbf{A} \mathbf{y})$, when
  implementing the expression in C++, the \emph{complexity} of the code will be very different. The left hand
  side has complexity $O(n^3)$, whilst the right hand size has complexity $O(n^2)$.

  Remark: even Eigen's own internal template expression mechanism is not able to efficiently
  exploit this fact.
\end{samsolution}
\end{samwriteprbpart}
\end{subproblem}

%%%%%%%%%%%% SUBPROBLEM 3
\begin{subproblem}{}
Write an \emph{efficient} C++ function
\begin{verbatim}
  void efficient_arrow_matrix_2_times_x(const VectorXd & d,
                              const VectorXd & a,
                              const VectorXd & x, VectorXd & y)
\end{verbatim}
that computes the same multiplication as in code \ref{mc:ArrowMatrixVector_arrowmatvec}, but
with optimal asymptotic complexity with respect to {$n$}. Compute the complexity of your code and
explain why you obtain the result.

Here \texttt{d} passes the vector $(d_{1},\ldots,d_{n})^{T}$ and \texttt{a} passes the
vector $(a_{1},\ldots,a_{n})^{T}$.
 \begin{samwriteprbpart}{sol}
\begin{samsolution}
  Due to the special structure of the matrix, it is possible to write a function that is more efficient
  than the standard matrix-vector multiplication.

  Define $\mathbf{a}_- := (a_{(1)}, \dots, a_{(n-1)}, 0)$.
  We can rewrite $\mathbf{A}$ as follows:

    \begin{align}
    \VA= & \begin{pmatrix}
      d_1 &     &        &         & a_1     \\
          & d_2 &        &         & a_2     \\
          &     & \ddots &         & \vdots  \\
          &     &        & d_{n-1}  & a_{n-1} \\
      a_1 & a_2 & \dots  & a_{n-1}   & d_n     \\
    \end{pmatrix} \\
    = & \mathrm{diag}(\mathbf{d}) + (\mathbf{0},\dots,\mathbf{0}, \mathbf{a}_-) + (\mathbf{0},\mathbf{0},\mathbf{0},\mathbf{a}_-)^\top
    =: \mathbf{D} + \mathbf{V} + \mathbf{H}.
    \end{align}
    and $\VA \Vx = \mathbf{D} \mathbf{x} + \mathbf{V} \mathbf{x} + \mathbf{H} \mathbf{x}$. Each of these
    multiplications can be done in $O(n)$ operations.

    See code listing \ref{mc:ArrowMatrixVector_arrowmatvec2}.
\vspace{0.5cm}

\samincludecpp{../Codes/MatVec/ArrowMatrix/solutions/arrowmatvec.cpp}[1]

\end{samsolution}
\end{samwriteprbpart}
\end{subproblem}

%%%%%%%%%%%% SUBPROBLEM 4
\begin{subproblem}{}
What is the complexity of your algorithm from sub-problem
\ref{subprb:ArrowMatrixVector_3} (with respect to matrix size $n$)?
 \begin{samwriteprbpart}{sol}
\begin{samsolution}
  The efficient implementation only needs two vector-vector element-wise multiplications and
  one vector-scalar multiplication. Therefore the complexity is $O(n)$.
\end{samsolution}
\end{samwriteprbpart}
\end{subproblem}

%%%%%%%%%%%% SUBPROBLEM 5

\begin{subproblem}{}
  Compare the runtime of your implementation and the implementation given in code
  \ref{mc:ArrowMatrixVector_arrowmatvec} for $n=2^{5,6,\ldots,12}$.
 \begin{samwriteprbpart}{sol}
 \begin{samhint}
Beware, for large $n$ ($n > 2048$) the computations may take a long time.
\end{samhint}
 \begin{samhint}
  You can use \verb|std::chrono| to time your code. HINT: TODO HOW TO USE CHRONO
 \end{samhint}
\begin{samsolution}
The standard matrix multiplication has runtime growing with $O(n^3)$.
The complexity of the more efficient implementation is with $O(n)$.
See \autoref{cpp:arrowmat_timing} and Figure~\ref{fig:arrowmatvec2timing}.

\begin{samcode}[C++11-code]{cpp:arrowmat_timing}{Computation of timing of arrow matrix multiplication}
\samincludecpp[label]{../Codes/MatVec/ArrowMatrix/solutions/arrowmatvec.cpp}[3]
\end{samcode}

\begin{figure}[ht]
\centering
\includegraphics[width=0.8\textwidth]{../Codes/MatVec/ArrowMatrix/arrowmatvec_comparison.eps}
\caption{Comparison of the timings of \texttt{arrow\_matrix\_2\_times\_x} and \texttt{efficient\_arrow\_matrix\_2\_times\_y} on an \texttt{Intel i7-6700k @ 4.00GHz}.}
\label{fig:arrowmatvec2timing}
\end{figure}

\end{samsolution}
\end{samwriteprbpart}
\end{subproblem}

\end{samproblem}
