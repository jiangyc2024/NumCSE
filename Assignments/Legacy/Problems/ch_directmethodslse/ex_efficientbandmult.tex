\renewcommand{\chpt}{ch_directmethodslse}

\begin{problem}[Banded matrix] \label{prb:efficientbandmult}
  For $n\in\bbN$ we consider the matrix

 \begingroup
\renewcommand*{\arraystretch}{1.5}
 \begin{align*}
  \vec{A} := \begin{bmatrix}
              2 & a_1 & 0 & \dots & \dots & \dots & 0 \\
              0 & 2 & a_2 & 0 & \dots & \dots & 0 \\
              b_1 & 0 & \ddots & \ddots & \ddots & & \vdots \\
              0 & b_2 & \ddots & \ddots & \ddots & \ddots & \vdots \\
              \vdots & 0 & \ddots & \ddots & \ddots & \ddots & 0 \\
              \vdots & \vdots  & \ddots & \ddots & \ddots & \ddots & a_{n-1} \\
              0 & 0 & \dots & 0 & b_{n-2} & 0 & 2 \\
             \end{bmatrix} \in \bbR^{n,n}
 \end{align*}
 \endgroup
 with $a_i, b_i \in \IR$. 

\emph{Remark.} The matrix $\VA$ is an instance of a banded matrix, see
\lref{sec:banded-matrices} and, in particular, the examples after 
\lref{def:bw}. However, you need not know any of the content of this
section for solving this problem.
 
\begin{subproblem}[2]
 Implement an \emph{efficient} \Cpp{} function:
 \begin{lstlisting}[language=c++]
  template <class Vector>
  void multAx(const Vector & a, const Vector & b, const Vector & x, Vector & y);
 \end{lstlisting}
 for the computation of $\vec{y} = \vec{A} \vec{x}$.
 
 \begin{solution}
  See \texttt{banded\_matrix.cpp}.
 \end{solution}
\end{subproblem}

\begin{subproblem}[4]\label{ebmm:sp3}
 Show that $\vec{A}$ is invertible if $a_i, b_i \in [0,1]$.
 
 \begin{hint}
   Give an indirect proof that $\ker \vec{A}$ is trivial, by looking at the
   largest (in modulus) component of an $\vec{x} \in \ker \vec{A}$.
 \end{hint}
 
\emph{Remark.} That $\VA$ is invertible can immediately be concluded from the
general fact that kernel vectors of irreducible, diagonally dominant matrices ($\to$
\lref{def:ddom}) must be multiples of $\left[1,1,\ldots,1\right]^{\top}$. Actually, the
proof recommended in the hint shows this fact first before bumping into a contradiction.

 \begin{solution}
 Assume by contradiction that $\ker \vec{A} \neq \{0\}$. Pick $0 \neq \vec{x} \in \ker \vec{A}$ and consider $i = \argmax \lvert x_j \rvert, x_i \neq 0$.
 Since $2 x_i + a_{i} x_{i+1} + b_{i-2} x_{i-2} = 0 \Rightarrow 2 \leq |
 \frac{x_{i+1}}{x_i} a_{i} + \frac{x_{i-2}}{x_i} b_{i-2}| < a_{i} +  b_{i-2}
 \leq 2$, unless $\vec{x} = const.$ (in which case $\vec{A}\vec{x} \neq 0$, as we
 see from the first equation). By contradiction $\ker \vec{A} = \{0\}$.
 \end{solution}
\end{subproblem}

\begin{subproblem}[3]
 Fix $b_i = 0, \forall i = 1, \dots, n-2$. Implement an efficient \Cpp{} function 
 \begin{lstlisting}[language=c++]
  template <class Vector>
  void solvelseAupper(const Vector & a, const Vector & r, Vector & x);
 \end{lstlisting}
 solving $\vec{A} \vec{x} = \vec{r}$.
 
 \begin{solution}
  See \texttt{banded\_matrix.cpp}.
 \end{solution}
\end{subproblem}

\begin{subproblem}[3] \label{subrb:implement_solvelseAupper}
For general $a_i, b_i \in [0,1]$ devise an efficient \Cpp{} function:
 \begin{lstlisting}[language=c++]
  template <class Vector>
  void solvelseA(const Vector & a, const Vector & b, const Vector & r, Vector & x);
 \end{lstlisting}
 that computes the solution of $\vec{A} \vec{x} = \vec{r}$ by means of Gaussian elimination. You cannot use any high level solver routines of \Eigen.
 
 \begin{hint}
   Thanks to the constraint $a_i, b_i \in [0,1]$, pivoting is not required in
   order to ensure stability of Gaussian elimination. This is asserted in
   \lref{lem:ddlu}, but you may just use this fact here. Thus, you
   can perform a straightforward Gaussian elimination from top to bottom
   as you have learned it in your linear algebra course. 
 \end{hint}

 \begin{solution}
  See \texttt{banded\_matrix.cpp}.
 \end{solution}
\end{subproblem}

\begin{subproblem}[1]
 What is the asymptotic complexity of your implementation of \verb|solvelseA| for $n \rightarrow \infty$.
 
 \begin{solution}
  To build the matrix we need at most $O(3n)$ insertions (3 per row). For the elimination stage we use three for loops, one of size $n$ and two of size, at most, 3 (exploiting the banded structure of $A$), thus $O(9n)$ operations. For backward substitution we use two loops, one of size $n$ and the other of size, at most, 3, for a total complexity of $O(3n)$. Therefore, the total complexity is $O(n)$.
 \end{solution}
\end{subproblem}

\begin{subproblem}[2]
 Implement \verb|solvelseAEigen| as in \ref{subrb:implement_solvelseAupper}, this time using \Eigen's sparse elimination solver.
 
 \begin{hint}
   The standard way of initializing a sparse \eigen{}-matrix efficiently, is via
   the triplet format as discussed in \lref{sec:eigensparse}. 
   You may also use direct initialization of a sparse matrix, provided that you 
   \verb|reserve()| enough space for the non-zero entries of each column, see 
   \href{http://eigen.tuxfamily.org/dox/group__TutorialSparse.html}{documentation}.
 \end{hint}

 \begin{solution}
  See \texttt{banded\_matrix.cpp}.
 \end{solution}
\end{subproblem}


\end{problem}

