% ncse_new/p1_SystemsOfEquations/ch2_DirectMethodsLSE/ex_PartitionedMatrix.tex
% solutions require:  solvelse.m

\begin{problem}[Partitioned Matrix \coreproblem] \label{prb:PartitionedMatrix}

Based on the block view of matrix multiplication presented in \lref{rem:blockops},
we looked a \emph{block elimination} for the solution of block partitioned linear
systems of equations in \lref{par:belim}. Also of interest are \lref{rem:BlockLU} and
\lref{rem:partlu} where LU-factorization is viewed from a block perspective. 
Closely related to this problem is \lref{ex:arrowlse}, which you should study
again as warm-up to this problem. 

Let the matrix $\VA\in\bbR^{n+1,n+1}$ be partitioned according to
\begin{align} \label{eq:block_matrix_lu}
\mathbf{A}=\left[
    \begin{array}{ccccccc}
      \mathbf{R}&\Vv\\
      \Vu^{T}&0
    \end{array}\right]\;,
\end{align}
where $\Vv\in\mathbb{R}^{n}$, $\Vu\in\mathbb{R}^{n}$, and
$\mathbf{R}\in\mathbb{R}^{n\times n}$ is \underline{upper triangular} and \underline{regular}.

\begin{subproblem}[1] \label{subprb:PartitionedMatrix_a}
  Give a necessary and sufficient condition for the triangular matrix $\vec{R}$ to be invertible.
  
  \begin{solution}
   $\vec{R}$ being upper triangular $\det (\vec{R}) = \prod_{i = 0}^n (\vec{R})_{i,i}$, means that all the diagonal elements must be non-zero for $\vec{R}$ to be invertible.
  \end{solution}

\end{subproblem}

\begin{subproblem}[2] \label{subprb:PartitionedMatrix_b}
  Determine expressions for the subvectors $\vec{z} \in \IR^n, \xi \in \IR$ of the solution vector of the linear system of equations
  \begin{align*}
   \begin{bmatrix}
      \mathbf{R} & \Vv\\
      \Vu^{T}  & 0 
   \end{bmatrix} \begin{bmatrix} \vec{z} \\ \xi \end{bmatrix} = \begin{bmatrix} \vec{b} \\ \beta \end{bmatrix}
  \end{align*}
  for arbitrary $\vec{b} \in \IR^n, \beta \in \IR$.

  \begin{hint}
   Use blockwise Gaussian elimination as presented in \lref{par:belim}. 
  \end{hint}


\begin{solution}
% Find the $\mathtt{LU}$-decomposition of $\mathbf{A}$.
% The $\mathtt{LU}$-decomposition is
% $$\mathbf{A}=\mathbf{LU}=\left(
%     \begin{array}{ccccccc}
%       \mathbf{I}&0\\
%       \Vu^{T}\mathbf{R}^{-1}&1
%     \end{array}\right)\left(
%     \begin{array}{ccccccc}
%       \mathbf{R}&\Vv\\
%  0&-\mathbf{u}^{T}\mathbf{R}^{-1}\mathbf{v}
%     \end{array}\right)$$
Applying the computation in \lref{rem:blockgs}, we obtain:
  \begin{align*}
   \begin{bmatrix}
      \vec{1} & 0 \\
      0  & 1
   \end{bmatrix} \begin{bmatrix} \vec{z} \\ \xi \end{bmatrix} = \begin{bmatrix}
   \vec{R}^{-1} ( \vec{b} - \vec{v} s^{-1} b_s )\\
   s^{-1} b_s
   \end{bmatrix}
  \end{align*}
    with $s := - (\vec{u}^\top \vec{R}^{-1} \vec{v}), b_{s} := (\beta - \vec{u}^\top \vec{R}^{-1} \vec{b})$.
\end{solution}
\end{subproblem}

\begin{subproblem}[2]
  Show that $\mathbf{A}$ is regular if and only if $\Vu^{T}\mathbf{R}^{-1}\Vv\neq0$.
  \begin{solution}
    The square matrix $\VA$ is regular, if the corresponding linear system has a
    solution for every right hand side vector. If $\Vu^{T}\mathbf{R}^{-1}\Vv\neq0$
    the expressions derived in the previous sub-problem show that a solution
    can be found for any $\Vb$ and $\beta$, because $\VR$ is already known to be
    invertible.
  \end{solution}
\end{subproblem}
  
\begin{subproblem}[3]
\label{subprb:PartitionedMatrix_d}
Implement the \Cpp{} function
\begin{lstlisting}[style=cppsimple]
 template <class Matrix, class Vector>
 void solvelse(const Matrix & R, const Vector & v, const Vector & u, const Vector & b, Vector & x);
\end{lstlisting}
for computing the solution of $\vec{A}\Vx=\Vb$ (with $\vec{A}$ as in 
\eqref{eq:block_matrix_lu}) efficiently. Perform size check on input matrices and vectors. 

\begin{hint}
 Use the decomposition from \ref{subprb:PartitionedMatrix_b}.
\end{hint}

\begin{hint}
 you can rely on the \verb|triangularView()| function to instruct \Eigen{} of the
 triangular structure of $\vec{R}$, see \lref{cpp:blockOps}. 
\end{hint}

\begin{hint}
 using the construct:
\begin{lstlisting}[style=cppsimple]
    typedef typename Matrix::Scalar Scalar; 
\end{lstlisting}
you can obtain the scalar type of the \verb|Matrix| type (e.g. \verb|double| for \verb|MatrixXd|). This can then be used as:
\begin{lstlisting}[style=cppsimple]
 Scalar a = 5;
\end{lstlisting}
\end{hint}

\begin{hint}
 using \verb|triangularView| and templates you may incur in weird compiling errors. If this happens to you, check \href{http://eigen.tuxfamily.org/dox/TopicTemplateKeyword.html}{http://eigen.tuxfamily.org/dox/TopicTemplateKeyword.html}
\end{hint}

\begin{hint}
 sometimes the C++ keyword \verb|auto| (only in std. C++11) can be used if you do not want to explicitly write the return type of a function, as in:
\begin{lstlisting}[style=cppsimple]
 MatrixXd a;
 auto b = 5*a;
\end{lstlisting}
\end{hint}

\begin{solution}
See \texttt{block\_lu\_decomp.cpp}.
% \lstinputlisting[caption={Partitioned matrix solver}, label={mc:PartitionedMatrix}]{\problems/ch_directmethodslse/MATLAB/solvelse.m}
\end{solution}
\end{subproblem}


\begin{subproblem}[3]
\label{subprb:PartitionedMatrix_e}
 Test your implementation by comparing with a standard LU-solver provided by \Eigen{}.
 
 \begin{hint}
  Check the page \href{http://eigen.tuxfamily.org/dox/group\_\_TutorialLinearAlgebra.html}{http://eigen.tuxfamily.org/dox/group\_\_TutorialLinearAlgebra.html}.
 \end{hint}
\begin{solution}
 See \texttt{block\_lu\_decomp.cpp}.
% \lstinputlisting[caption={Partitioned matrix solver}, label={mc:PartitionedMatrix}]{\problems/ch_directmethodslse/MATLAB/solvelse.m}
\end{solution}
\end{subproblem}

\begin{subproblem}[2]
\label{subprb:PartitionedMatrix_f}
 What is the asymptotic complexity of your implementation of \verb|solvelse()| in terms of problem size parameter $n \rightarrow \infty$?
\end{subproblem}

\begin{solution}
 The complexity is $O(n^2)$. The backward substitution for $\vec{R}^{-1} \vec{x}$ is $O(n^2)$, vector dot product and subtraction is $O(n)$, so that the complexity is dominated by the backward substitution $O(n^2)$.
\end{solution}
\end{problem}
