% ncse_new/p3_ODE/ch1_SingleStepMethods/ex_TaylorODE.tex
% IT RELIES ON ex_RK3, 3-stage Runge-Kutta problem
% vector x in pred_prey is very hard to understand, everything should be rewritten in a decent way
% solutions:   Taylor_step.m

\begin{problem}[Integrating ODEs using the Taylor expansion method] \label{prb:TaylorODE}
 
In \ncseref{cha:ssm} of the course we studied single step methods for the integration of initial value problems for ordinary differential equations $\dot{\Vy}=\Vf(\Vy)$, \ncsedef{def:esv}.
Explicit single step methods have the advantage that they only rely on point evaluations of the right hand side $\Vf$.

  This problem examines another class of methods that is obtained by
  the following reasoning: if the right hand side
  $\Vf:\bbR^n\to\bbR^n$ of an autonomous initial value problem
  \begin{equation}    \label{eq:exTaylor_ODE}
    \dot{\Vy} = \Vf(\Vy)\;, \qquad \Vy(0)=\Vy_0\;,
  \end{equation}
  with solution $\Vy:\bbR\rightarrow\bbR^n$ is smooth, also the solution $\Vy(t)$ will be regular and it is possible to expand it
  into a Taylor sum at $t=0$, see \ncsethm{thm:taylor},
  \begin{equation}
    \label{eq:exTaylor}
    \Vy(t) = \sum\limits_{n=0}^m \frac{\Vy^{(n)}(0)}{n!}t^n + R_{m}(t)\;,
  \end{equation}
  with remainder term $R_{m}(t) = O(t^{m+1})$ for $t\to 0$.
  
  A single step method for the numerical integration of \eqref{eq:exTaylor_ODE} can be
  obtained by choosing $m=3$ in \eqref{eq:exTaylor}, neglecting the remainder term, and
  taking the remaining sum as an approximation of $\Vy(h)$, that is,
  \begin{gather*} 
    \Vy(h) \approx \Vy_{1} := \Vy(0) + \frac{d\Vy}{dt}(0)h + 
    \frac{1}{2}\frac{d^{2}\Vy}{dt^{2}}(0)h^{2}
    + \frac{1}{6}\frac{d^{3}\Vy}{dt^{3}}(0)h^{3}\;.
  \end{gather*}
  Subsequently, one uses the ODE and the initial condition to replace
  the temporal derivatives $\frac{d^{l}\Vy}{dt^{l}}$ with expressions
  in terms of (derivatives of ) $\Vf$. This yields a single step 
  integration method called \emph{Taylor (expansion) method}.

%=====================================================================================================

\begin{subproblem}[3] \label{subprb:TaylorODE_1}     % a
Express $\frac{d\Vy}{dt}(t)$ and $\frac{d^{2}\Vy}{dt^{2}}(t)$ in terms of $\Vf$ and its Jacobian $\VD\Vf$.

\begin{hint}
Apply the chain rule, see \ncserem{rem:diff}, then use the ODE \eqref{eq:exTaylor_ODE}.
\end{hint}

\begin{solution}
For the first time derivative of $\Vy$, we just use the differential equation:
$$\frac{d\Vy}{dt}(t) = \Vy'(t) = \Vf(\Vy(t)).$$ 
For the second derivative, we use the previous equation and apply chain rule and then once again insert the ODE:
\begin{equation*}
\frac{d^2\Vy}{dt^2}(t) = \frac{d}{dt}\Vf(\Vy(t)) = \VD\Vf(\Vy(t))\cdot\Vy'(t) = \VD\Vf(\Vy(t))\cdot\Vf(\Vy(t)).
\end{equation*}
Here $\VD\Vf(\Vy(t))$ is the Jacobian of $\Vf$ evaluated at $\Vy(t)$.
\end{solution}
\end{subproblem}

%=====================================================================================================

\begin{subproblem}[5] \label{subprb:TaylorODE_2}     % b
Verify the formula 
\begin{gather} \label{eq:exTaylor3deriv}
\frac{d^{3}\Vy}{dt^{3}}(0) =   \VD^{2}\Vf(\Vy_{0})\big(\Vf(\Vy_{0}),\Vf(\Vy_{0})\big) + \VD\Vf(\Vy_{0})^{2}\Vf\big(\Vy_{0}\big)\;.
\end{gather}

\begin{hint}
this time we have to apply both the product rule \ncseeq{eq:productrule} and chain rule \ncseeq{eq:chainrule} to the expression derived in the previous sub-problem.
\end{hint}

To gain confidence, it is advisable to consider the scalar case $d=1$ first, where $f:\bbR\to\bbR$ is a real valued function. 

Relevant for the case $d>1$ is the fact that the first derivative of $\Vf$ is a linear mapping $\VD\Vf(\Vy_{0}): \bbR^n\to \bbR^n$.
This linear mapping is applied by multiplying the argument with the Jacobian of $\Vf$.
Similarly, the second derivative is a \textit{bilinear} mapping $\VD^{2}\Vf(\Vy_{0}): \bbR^n\times \bbR^n\to \bbR^n$.
The $i$-th component of $\VD^{2}\Vf(\Vy_{0})\big(\Vv, \Vv\big)$ is given by
\begin{equation*}
\VD^{2}\Vf(\Vy_{0})\big(\Vv, \Vv\big)_i = \Vv^T \VH\Vf_i(\Vy_{0})\Vv,
\end{equation*}
where $\VH f_i(\Vy_{0})$ is the Hessian of the $i$-th component of $\Vf$ evaluated at $\Vy_{0}$.

\begin{solution}
We follow the hint and first have a look at the scalar case.
Here, the Jacobian reduces to $f'(y(t))$. Thus, we have to calculate 
$$\frac{d^3y}{dt^3}(t) = \frac{d}{dt}\bigl(f'(y(t))f(y(t))\bigr)\ .$$
Product rule and chain rule give
$$\frac{d}{dt}\bigl(f'(y(t))f(y(t))\bigr) = f''(y(t))y'(t)f(y(t)) + f'(y(t))f'(y(t))y'(t).$$
Inserting the ODE $y'(t)=f(y(t))$ once again yields
$$ \frac{d^3y}{dt^3}(t) = f''(y(t))f(y(t))^2 + f'(y(t))^2f(y(t)). $$
This already resembles Formula~\ref{eq:exTaylor3deriv}.
The first term is quadratic in $f(y(t))$ and involves the second derivative of $f$, whereas the second term involves the first derivative of $f$ in quadratic form.

\vspace{4mm}
To understand the formula for higher dimensions, we verify it componentwise.
For each component $y_i(t)$ we have a function $f_i: \mathbb{R}^n \to \mathbb{R}$.

For the first derivative of $y_i(t)$, this is straightforward:
$$ y_i'(t) = f_i(\Vy(t)).$$
Thus, the second derivative is 
\begin{align*}
y_i''(t)  &= \frac{d}{dt} \big(f_i(\Vy(t))\big) =  \sum_{j=1}^n \partial_{y_j}f_i(\Vy(t)) \; y'_j(t)\\
% =  \sum_{j=1}^n \frac{df(\Vy(t))}{dy_j} f_j(\Vy(t))
& =  \bigl(\grad f_i(\Vy(t))\bigr)^T \cdot \Vy'(t)  = \bigl(\grad f_i(\Vy(t))\bigr)^T \cdot \Vf(\Vy(t))\ .
\end{align*}
Building up all components gives us what we have already obtained in \ref{subprb:TaylorODE_1}:
$$    \Vy''(t) = \VD\Vf(\Vy(t))\cdot \Vf(\Vy(t)),$$
with the Jacobian $\VD\Vf(\Vy(t))$, which contains the gradients of the components of $f$ row-wise.

Now, we apply product rule to $y_i''(t)$ to obtain
$$ y_i'''(t) = \left(\frac{d}{dt}\bigl(\grad f_i(\Vy(t))\bigr)^T \right)\cdot\Vy'(t) + \bigl(\grad f_i(\Vy(t))\bigr)^T \cdot\Vy''(t).$$
The second term of the sum again builds up to
$$ \VD\Vf(\Vy(t))\cdot\Vy''(t) = \VD\Vf(\Vy(t))\cdot\VD\Vf(\Vy(t))\cdot\Vf(\Vy(t)) = \VD\Vf(\Vy(t))^2\cdot\Vf(\Vy(t))\ .$$
For the first term, we first write the scalar product of the two vectors as a sum and then interchange the order of derivatives.
This is possible as long as functions are sufficiently differentiable.
$$ \left(\frac{d}{dt} \bigl(\grad f_i(\Vy(t))\bigr)^T\right)\cdot\Vy'(t) 
= \sum_{j=1}^n \left(\partial_{y_j}\frac{d}{dt}f_i(\Vy(t))\right)f_j(\Vy(t))\ .$$
Now we apply the chain rule:
\begin{align*}
\sum_{j=1}^n\left(\partial_{y_j}\frac{d}{dt}f_i(\Vy(t))\right) f_j(\Vy(t))
    =& \sum_{j=1}^n \left(\partial_{y_j}\sum_{l=1}^n \partial_{y_l}f_i(\Vy(t))y_l'(t)\right)f_j(\Vy(t))\\
    =& \sum_{j,l=1}^n\left(\partial_{y_j}\partial_{y_l}f_i(\Vy(t))\right) f_l(\Vy(t))f_j(\Vy(t))\\
    =& \Vf(\Vy(t))^T \cdot \VH f_i(\Vy(t)) \cdot \Vf(\Vy(t)).
  \end{align*}
This is the desired result.
\end{solution}
\end{subproblem}

%=====================================================================================================

\begin{subproblem}[4] \label{subprb:TaylorODE_3}     % c
We now apply the Taylor expansion method introduced above to the \emph{predator-prey} model \eqref{eq:predator-prey} introduced in \ref{prb:RK3} and \ncseref{ex:LV}.
    
To that end write a header-only \Cpp{} class \verb|TaylorIntegrator| for the integration of the autonomous ODE of \eqref{eq:predator-prey} using the Taylor expansion method with uniform time steps on the temporal interval $[0, 10]$.

\begin{hint}
 You can copy the implementation of \ref{prb:RK3} and modify only the \verb|step| method to perform a single step of the Taylor expansion method.
\end{hint}

\begin{hint}
 Find a suitable way to pass the data for the derivatives of the r.h.s. function $\vec{f}$ to the \verb|solve| function. You may modify the signature of \verb|solve|.
\end{hint}

\begin{hint}
 See \verb|taylorintegrator_template.hpp|.
\end{hint}

\cprotEnv \begin{solution}
 For our particular example, we have
  \begin{align*}
    \Vy &=        \begin{pmatrix}      u\\      v    \end{pmatrix},
    &\Vf(\Vy) &=    \begin{pmatrix}      (\alpha_1 - \beta_1y_2)y_1\\      -(\alpha_2 - \beta_2y_1)y_2    \end{pmatrix},\\
    \VD\Vf(\Vy) &=    \begin{pmatrix}      \alpha_1 - \beta_1y_2 & -\beta_1y_1\\      \beta_2y_2 &-(\alpha_2 - \beta_2y_1)    \end{pmatrix},&&\\
    \VH f_1(\Vy) &=  \begin{pmatrix}      0 & -\beta_1\\      -\beta_1 & 0    \end{pmatrix},
    &\VH f_2(\Vy) &= \begin{pmatrix}      0 & \beta_2\\      \beta_2 & 0    \end{pmatrix}.
  \end{align*}
  
 See \verb|taylorintegrator.hpp|.
%     To implement Taylor's method, we use again the wrapper  function~\ref{mc:MyIntegrate} created in \ref{prb:RK3}
%     We only have to implement a new step function \texttt{Taylor\_step}.
% 
% %     \lstinputlisting[caption={Single step for the Taylor method},label={mc:Taylor_step}]
% %     {p3_ODE/ch1_SingleStepMethods/MATLAB/Taylor_step.m}
% 
%     Here, we assume, that the ODE file can apply the linear mapping defined by multiplying the Jacobian $\VD\Vf(\Vy)$ to a vector $\Vx$ by calling
%     \begin{center}     \texttt{Dfx = odefun(t, y, 'df', x)}.    \end{center}
%     Additionally we want the ODE file to compute the quadratic form which corresponds to the second derivative applied to a vector $\Vx$,
%     $\VD^2\Vf(\Vy)(\Vx,\Vx)$, by calling
%     \begin{center}        \texttt{D2fx = odefun(t, y, 'd2f', x)}.    \end{center}
%     The ODE file given in Listing~\ref{mc:pred_prey} includes these two additional calls.
% 
%     The application of Taylor's method to this example is done in code~\ref{mc:exRK3} inside the \texttt{for}-loop.
\end{solution}
\end{subproblem}

%=====================================================================================================

\begin{subproblem}[1] \label{subprb:TaylorODE_4}     % d
Experimentally determine the order of convergence of the considered Taylor expansion method when it is applied to solve \eqref{eq:predator-prey}.
Study the behaviour of the error at final time $t=10$ for the initial data $\vec{y}(0) = [100,5]$.
    
As a reference solution use the same data as \ref{prb:RK3}.

\begin{hint}
 See \verb|taylorprey_template.cpp|.
\end{hint}

\cprotEnv \begin{solution}
From \verb|taylorprey.cpp|, we see cubic algebraic convergence $O(h^3)$.
\end{solution}
\end{subproblem}

%=====================================================================================================

\begin{subproblem}[1] \label{subprb:TaylorODE_5}     % e
What is the disadvantage of the Taylor method compared with a Runge-Kutta method?

\begin{solution} As we can see in the error table, the error of the studied Runge-Kutta method and Taylor's method are practically identical.
The obvious disadvantage of Taylor's method in comparison with Runge-Kutta methods is that the former involves rather complicated higher derivatives of $f$.
If we want higher order, those formulas get even more complicated, whereas explicit Runge-Kutta methods work with only a few evaluations of $f$ itself, yielding results which are comparable. Moreover, the Taylor expansion method cannot be applied for $f$, when this is given in procedural form.
\end{solution}
\end{subproblem}

\end{problem}









