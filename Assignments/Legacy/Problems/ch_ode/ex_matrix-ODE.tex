\begin{problem}[Matrix-valued Differential Equation \coreproblem]\label{prb:matrix-diff-eq} 

First we consider the \emph{linear} matrix differential equation  
\begin{align} \label{eq:ivp_mat_A}
  \dot{\VY} = \VA \VY =: \Vf(\vec{Y}) \quad \text{with} \quad \VA \in
  \bbR^{n\times n}.
\end{align}
whose solutions are \emph{matrix-valued functions} $\VY: \bbR \to \bbR^{n\times n}$.

\begin{subproblem}[1] \label{sp:cons}
% Compute $\frac{d}{dt}(\VY^\top \VY)$ and 
Show that for \emph{skew-symmetric} $\VA$, i.e. $\VA = - \VA^\top$ we have:
\begin{align*}
  \VY(0) \:\text{ orthogonal} \quad\Longrightarrow\quad \VY(t) \:\text{
  orthogonal}\quad
  \forall t\;.
\end{align*}

\begin{hint}
  Remember what property distinguishes an orthogonal matrix. Thus you see that the
  assertion we want to verify boils down to showing that the bilinear expression
  $t\mapsto \VY(t)^{\top}\VY(t)$ does not vary along trajectories, that is, its
  time derivative must vanish. This can be established by means of the product
  rule \lref{eq:productrule} and using the differential equation.
\end{hint}

\begin{solution}
Let us consider the time derivative of $\VY^\top \VY$:
\begin{align*}
  \frac{d}{dt}(\VY^\top \VY)& = \dot{\VY}^\top \VY +
                              \VY^\top \dot{\VY} \\
                            & = (\VA \VY)^\top \VY + \VY^\top
                              \VA \VY \\
                            &= - \VY^\top \VA \VY + \VY ^\top \VA \VY \\
                            &= 0
\end{align*}
This implies $\VY(t)^\top \VY(t)$ is constant. From this, it follows that the orthogonality is preserved, as claimed. In fact $\vec{I} = \VY(0)^\top \VY(0) = \VY(t)^\top \VY(t)$.
\end{solution}

\end{subproblem}

\begin{subproblem}[1] \label{sp:implement}
Implement three \Cpp{} functions
\begin{enumerate}[(i)]
  	\item a single step of the explicit Euler method: %\ncseref{sec:das-expl-eulerv}:
\begin{lstlisting}[language=c++]
Eigen::MatrixXd eeulstep(const Eigen::MatrixXd & A, const Eigen::MatrixXd & Y0, double h);
\end{lstlisting}
  	\item a single step of the implicit Euler method: %\ncseref{sec:das-impl-eulerv},
\begin{lstlisting}[language=c++]
Eigen::MatrixXd ieulstep(const Eigen::MatrixXd & A, const Eigen::MatrixXd & Y0, double h);
\end{lstlisting}
  	\item a single step of the implicit mid-point method: %\ncseref{sec:impl-mitt}.
\begin{lstlisting}[language=c++]
Eigen::MatrixXd impstep(const Eigen::MatrixXd & A, const Eigen::MatrixXd & Y0, double h); 
\end{lstlisting}
\end{enumerate}
which determine, for a given initial value $\VY(t_0) = \VY_0$ and for given step size $h$, approximations for $\VY(t_0+h)$ using one step of the corresponding method for the approximation of the ODE \eqref{eq:ivp_mat_A}

\begin{solution}
The explicit Euler method is given by 
\begin{equation*}
		\VY_{k+1} = \VY_k + h \Vf(t_k, \VY_k).
\end{equation*}
For the given differential equation we therefore obtain 
\begin{equation*}
	\VY_{k+1} = \VY_k + h \VA\VY_k.
\end{equation*}
The implicit Euler method is given by
\begin{equation*}
		\VY_{k+1} = \VY_k + h \Vf(t_{k+1}, \VY_{k+1}).
\end{equation*}
For the given differential equation this yields
\begin{equation*}
	\VY_{k+1} = \VY_k + h \VA\VY_{k+1}
	\quad \Longrightarrow \quad
	\VY_{k+1} = (\VI - h \VA)^{-1} \VY_k
\end{equation*}
Finally, the implicit mid-point method is given by
\begin{equation*}
	\VY_{k+1} = \VY_k + h \Vf\bigl(\tfrac{1}{2}(t_k + t_{k+1}), \tfrac{1}{2} (\VY_k + \VY_{k+1})\bigr),
\end{equation*}
hence
\begin{equation*}
	\VY_{k+1} = \VY_k + \tfrac{h}{2} \VA (\VY_k + \VY_{k+1})
	\quad \Longrightarrow \quad
	\VY_{k+1} = \bigl(\VI - \tfrac{h}{2} \VA\bigr)^{-1} \bigl(\VY_k + \tfrac{h}{2} \VA \VY_k\bigr).
\end{equation*}
\end{solution}
\end{subproblem}

\begin{subproblem}[1]
  Investigate numerically,
  which one of the implemented methods preserves orthogonality in the sense of
  sub-problem \ref{sp:cons} for the ODE \eqref{eq:ivp_mat_A} and which one
  doesn't. To that end, consider the matrix
 \begin{gather*}
   \vec{M} := 
   \begin{bmatrix}
     8&1&6\\3&5&7\\9&9&2
   \end{bmatrix}
 \end{gather*}
 and use the matrix $\vec{Q}$ arising from the QR-decomposition of $\vec{M}$ as initial data $\vec{Y}_0$.
 As matrix $A$, use the skew-symmetric matrix
 \[
 \vec{A} = \begin{bmatrix}
   0&1&1\\-1&0&1\\-1&-1&0
 \end{bmatrix}\;.
 \]
 To that end, perform $n = 20$ time steps of size $h = 0.01$ with each method and compute the Frobenius norm of $\vec{Y}(T)' \vec{Y}(T) - \vec{I}$.
 Use the functions from subproblem \ref{sp:implement}. 
 
\cprotEnv\begin{solution}
 Orthogonality is preserved only by the implicit midpoint rule. Explicit and implicit Euler methods do not preserve orthogonality. See \verb|matrix_ode.cpp|.
\end{solution}

From now we consider a non-linear ODE that is structurally similar to \eqref{eq:ivp_mat_A}.
We study the initial value problem
    \begin{align}
      \label{eq:diffeq}
      \dot{\VY} = -(\VY-\VY^{\top})\VY =: f(\VY)\quad,\quad
      \VY(0) = \VY_{0}\in\bbR^{n,n},
    \end{align}
    whose solution is given by a \emph{matrix-valued function}
    $t\mapsto \VY(t) \in \bbR^{n \times n}$.

\begin{subproblem}[1] \label{sp:odesolve}
 Write a \Cpp{} function 
 \begin{lstlisting}[language=c++]
Eigen::MatrixXd matode(const Eigen::MatrixXd & Y0, double T)
 \end{lstlisting}
% \begin{center}
%   \texttt{function YT = matode(Y0,T)} 
% \end{center}
which solves \eqref{eq:diffeq} on $[0,T]$ using the \Cpp{} header-only class
\verb|ode45| (in the file \verb|ode45.hpp|). The initial value should be given by
a $n \times n$ \Eigen{} matrix \verb|Y0|. Set the absolute tolerance to $10^{-10}$
and the relative tolerance to $10^{-8}$. The output should be an approximation of
$\VY(T) \in \bbR^{n \times n}$.

\begin{hint}
The \verb|ode45| class works as follows:
\begin{enumerate}
 \item Call the constructor, and specify the r.h.s function $f$ and the type for the solution and the initial data in \verb|RhsType|, example:
 \begin{lstlisting}[language=c++]
ode45<StateType> O(f);
 \end{lstlisting}
 with, for instance, \verb|Eigen::VectorXd| as \verb|StateType|.
 \item (optional) Set custom options, modifying the \verb|struct| \verb|options| inside \verb|ode45|, for instance:
 \begin{lstlisting}[language=c++]
O.options.<option_you_want_to_change> = <value>;
 \end{lstlisting}
 \item Solve the IVP and store the solution, e.g.:
 \begin{lstlisting}[language=c++]
std::vector<std::pair<Eigen::VectorXd, double>> sol = O.solve(y0, T);
 \end{lstlisting}
\end{enumerate}
Relative and absolute tolerances for \verb|ode45| are defined as \verb|rtol|
resp. \verb|atol| variables in the \verb|struct| \verb|options|. The return value
is a sequence of states and times computed by the adaptive single step method.
\end{hint}

\begin{hint}
The type \verb|RhsType| needs a vector space structure implemented with operators \verb|*|, \verb|*|,  \verb|*=|, \verb|+=| and assignment/copy operators. Moreover a norm method must be available. Eigen vector and matrix types, as well as fundamental types are eligible as \verb|RhsType|.
\end{hint}

\begin{hint}
 Have a look at the public interface of \verb|ode45.hpp|. Look at the template file \verb|matrix_ode_template.cpp|.
\end{hint}

\cprotEnv \begin{solution}
 The class \verb|ode45| can take \verb|Eigen::MatrixXd| as \verb|StateType|. Alternatively, one can transform Matrices to Vectors (and viceversa), using the \verb|Eigen::Map| function (similar to \Matlab{} own \verb|reshape| function). See \verb|matrix_ode.cpp|.
\end{solution}

\end{subproblem}

\begin{subproblem}[2] \label{sp:const}
Show that the function $t\mapsto \VY^{\top}(t)\VY(t)$ is constant for the exact solution $\VY(t)$ of \eqref{eq:diffeq}.

\begin{hint}
  Remember the general product rule \lref{eq:productrule}. 
\end{hint}

\begin{solution}
By the product rule and using the fact that $\vec{Y}$ is a solution of the IVP we obtain
\begin{align*}
\frac{d}{dt} (\VY^\top(t)\VY(t)) = & \frac{d}{dt} (\VY^\top(t)) \VY(t) + \VY^\top(t) \frac{d}{dt}(\VY(t)) \\
= & ( -(\VY(t) - \VY^\top(t)) \VY(t) )^\top \VY(t) + \VY(t) ( -(\VY(t) - \VY^\top(t)) \VY(t) ) \\
= & -\VY^\top(t) \VY^\top(t) \VY(t) + \VY^\top \VY(t) \VY(t) \\
& - \VY^\top \VY(t) \VY(t) + \VY^\top(t) \VY^\top(t) \VY(t) = 0.
\end{align*}
implying that the map is constant.
\end{solution}

\end{subproblem}

\begin{subproblem}[1] \label{sp:checkinvariant}
Write a \Cpp{} function 
\begin{lstlisting}[language=c++]
bool checkinvariant(const Eigen::MatrixXd & M, double T);
\end{lstlisting}
which (numerically) determines if the statement from \ref{sp:const} is true, for
$t=T$ and for the output of \verb|matode| from sub-problem \ref{sp:odesolve}. You
must take into account round-off errors. The function's input should be the same
as that of \verb|matode|.

\begin{hint}
 See \verb|matrix_ode_template.cpp|.
\end{hint}

\cprotEnv \begin{solution}
 Let $\vec{Y}_k$ be the output of \verb|matode|. We compute $\vec{Y}_0^\top \vec{Y}_0 - \vec{Y}_k^\top \vec{Y}_k$ and check if the (Frobenius) norm is smaller than a constant times the machine \verb|eps|. Even for an orthogonal matrix, we have to take into account round-off errors. See \verb|matrix_ode.cpp|.
\end{solution}

\end{subproblem}

\begin{subproblem}[1] \label{sp:checkinvariant}
 Use the function \verb|checkinvariant| to test wether the invariant is preserved by \verb|ode45| or not. Use the matrix $\vec{M}$ defined above and
 and $T = 1$.
 
 \cprotEnv \begin{solution}
  The invariant is not preserved. See \verb|matrix_ode.cpp|.
 \end{solution}

\end{subproblem}


% \begin{subproblem}[1]
%  
% The so-called discrete gradient rule for \ref{eq:diffeq} is a single-step method given by: 
% \begin{align*}
% % \label{eq:gradmeth}
% \VY_{\ast} = \VY_{k} + \tfrac{1}{2}h_k f(\VY_{k})\quad,\quad
% \VY_{k+1} = (\VI+\tfrac{1}{2}h_k(\VY_{\ast}-\VY^{\top}_{\ast}))^{-1}
% (\VI-\tfrac{1}{2}h_k(\VY_{\ast}-\VY^{\top}_{\ast}))\VY_{k},
% \end{align*}
% where $\VY_k$ is the approximated solution at $t=t_k$ and $h_k$ is the stepsize, i.e. $h_k=t_{k+1}-t_k$
% 
% Write a \Cpp{} function
% \begin{center}
% \texttt{function YT = matodespr (Y0,T,N)}
% \end{center}
% which approximates a solution of the ODE \ref{eq:diffeq} with the discrete gradient rule \ref{eq:gradmeth}. The input and output are the same as in the \texttt{matode} routine from \ref{sp:odesolve}. The additional parameter \texttt{N} gives the number of equidistant integration steps.\\
% 
% \begin{solution}
%  
% \end{solution}
% \end{subproblem}
% 
% \begin{subproblem}[1]
% 
% Write a \Cpp{} function
% \begin{center}
% \texttt{function matodecvg ()}
% \end{center}
% that determines the convergence rate of the discrete gradient rule in a numerical experiment.
% 
% Use $N$ equidistant integration steps 
% \begin{center}
% $N\in\{10,20,40,80,160,320,640,1280\}$
% \end{center}
% to approximate a solution of \ref{eq:diffeq}.
% Create an appropriate error vs. number of integration steps plot.
% 
% Instead of the exact solution use the solution given by the \texttt{matode} function from \ref{sp:odesolve}.
% 
% Use $T=1$ as the end point in the numerical experiment. Use the \emph{orthogonal} matrix \texttt{Y0}, which is given by the result of
% \begin{center}
% 	\texttt{[Y0,dummy] =
% 	qr(magic(3))}.
% \end{center}
% as the initial value for $\VY_{0}$.
% 
% 
%  \begin{solution}
% See \ref{code:convrate} and \ref{fig:convrate}.
% \end{solution}
% 
% \end{subproblem}
% 
% \begin{subproblem}[1]
%  
% 
% Prove the convergence rate of the discrete gradient rule for \ref{eq:diffeq} observed in \ref{sp:convrate} by finding its maximal order of consistency.
% \begin{hint}
%  For a matrix $\VA$ with $\norm{\VA}<1$ we get $(\VI-\VA)^{-1}=\sum_{ n=0}^{\infty} \VA^n$.
% \end{hint}
% 
%  \begin{solution}
% % From \ref{sp:convrate} we know that the discrete gradient rule has a maximal consistency order of 2. As the initial value problem is autonomous, we only have to show
% % \[
% % 	\norm{\Phi^h\VY - \Psi^h\VY}=O(h^3).
% % \]
% % The Taylor series of $\Phi^h\VY_0$ is
% % \[
% % 	\Phi^h\VY=\VY + hf(\VY) +\frac{h^2}{2} D{t} \big(f(\VY)\big)+O(h^3).
% % \]
% % With
% % \begin{align*}
% % 	D{t} \big(f(\VY)\big) &= D{t} \big(-(\VY-\VY^\top )\VY\big)\\
% % 	&=D{t} \big(\VY^\top \VY-\VY\VY\big)\\
% % 	&=\dot{\VY}^\top \VY+\VY^\top \dot{\VY} -\dot{\VY}\VY-\VY\dot{\VY}\\
% % 	&=(\VY^\top \VY-\VY\VY)\VY+\VY^\top (\VY^\top \VY-\VY\VY)-(\VY^\top \VY-\VY\VY)\VY-\VY(\VY^\top \VY-\VY\VY)\\
% % 	&=\VY^\top \VY\VY-\VY^\top \VY^\top \VY+\VY^\top \VY^\top \VY-\VY^\top \VY\VY-\VY^\top \VY\VY+\VY\VY\VY-\VY\VY^\top \VY+\VY\VY\VY\\
% % 	&= 2\VY^3-\VY^\top \VY\VY-\VY\VY^\top \VY
% % \end{align*}
% % we get
% % \[
% % 	\Phi^h\VY=\VY_0 + hf(\VY) +\frac{h^2}{2} (2\VY^3-\VY^\top \VY\VY-\VY\VY^\top \VY)+O(h^3).
% % \]
% % Now our goal is to show that
% % \[
% % 	\Psi^h\VY=\VY + hf(\VY) +\frac{h^2}{2} (2\VY^3-\VY^\top \VY\VY-\VY\VY^\top \VY)+O(h^3).
% % \]
% % First we write
% % \begin{align*}
% % 	\VY_{*}&=\VY+\frac{h}{2}f(\VY)= \VY-\frac{h}{2}(\VY-\VY^\top )\VY.
% % \end{align*}
% % Then we define
% % \begin{align*}
% % 	\VA&:=\frac{h}{2}(\VY_*-\VY_*^\top )\\
% % 	&=\frac{h}{2}(\VY-\VY^\top )-\frac{h^2}{4}\big((\VY-\VY^\top )\VY-\VY^\top (\VY^\top -\VY)\big)\\
% % 	&=\frac{h}{2}(\VY-\VY^\top )-\frac{h^2}{4}(\VY^2-\VY^\top \VY^\top ).
% % \end{align*}
% % Now we can rewrite
% % \begin{align*}
% % 	\Psi^h\VY&= (\VI+\tfrac{1}{2}h(\VY_{\ast}-\VY^{\top}_{\ast}))^{-1}(\VI-\tfrac{1}{2}h(\VY_{\ast}-\VY^{\top}_{\ast})\VY =(\VI+\VA)^{-1}(\VI-\VA)\VY\\
% % 	&=\sum_{n=0}^\infty(-\VA)^n(\VI-\VA)\VY =\sum_{n=0}^\infty\big( (-\VA)^n\VY+(-\VA)^{n+1}\VY \big)\\
% % 	&=\VY+\sum_{n=1}^\infty2 (-\VA)^n\VY =\VY-2\VA\VY+2\VA^2\VY+O(h^3).
% % \end{align*}
% % As
% % \begin{align*}
% % 	\VA^2&=\parens[\Big]{\frac{h}{2}(\VY-\VY^\top )-\frac{h^2}{4}(\VY^2-\VY^\top \VY^\top )} \parens[\Big]{\frac{h}{2}(\VY-\VY^\top )-\frac{h^2}{4}(\VY^2-\VY^\top \VY^\top )}\\
% % 	&=\frac{h^2}{4}(\VY-\VY^\top )^2+O(h^3),
% % \end{align*}
% % we conclude that
% % \begin{align*}
% % 	\Psi^h\VY&= \VY-2\frac{h}{2} \parens[\Big]{(\VY-\VY^\top )-\frac{h^2}{4}(\VY^2-\VY^\top \VY^\top )}\VY+2\frac{h^2}{4}(\VY-\VY^\top )^2\VY+O(h^3)\\
% % 	&=\VY+\frac{h}{2}f(\VY)+\frac{h^2}{2}\parens[\big]{\VY^3-\VY^\top \VY^\top \VY+\VY^3-\VY\VY^\top \VY-\VY^\top \VY^2+\VY^\top \VY^\top \VY}+O(h^3)\\
% % 	&=\VY+\frac{h}{2}f(\VY)+\frac{h^2}{2}(2\VY^3-\VY\VY^\top \VY-\VY^\top \VY^2)+O(h^3)\\
% % 	&=\VY+\frac{h}{2}f(\VY)+\frac{h^2}{2}D{t} \big(f(\VY)\big) +O(h^3) =\Phi^h\VY+O(h^3).
% % \end{align*}
% \end{solution}


\end{subproblem}


% This exercise is a follow-up to \autoref{prb:matrix-diff-eq}.
% The goal is to approximate the solution of the autonomous initial value problem
% \begin{equation}
%   \label{eq:diffeq}
%   \dot{\VY} = -(\VY-\VY^{\top})\VY =: f(\VY),\quad
%   \VY(0) = \VY_{0}\in\bbR^{n \times n},
% \end{equation}
% whose solution is a matrix-valued function $t\mapsto \VY(t) \in \bbR^{n \times n}$, with  the implicit mid-point rule (\ncseeq{eq:imidp}). As implicit methods require the solving of an equation, we will use Newton's method.
% 
% \begin{subproblem}[1] \label{sp:stepsize}
% Let $\BPsi:\bbR \times \bbR^{d \times d} \mapsto \bbR^{d \times d}$ be the discrete evolution operator ($\to$ \ncsedef{def:esv}) to the implicit mid-point rule \ncseeq{eq:imidp} applied to the autonomous matrix differential equation \eqref{eq:diffeq}. Note that, for an autonomous differential equation, we only need to give the evolution operator one step size argument instead of two ``time arguments'' $(s,t)$.
% 
% Find a function 
% $\BF_{h}:\bbR^{d \times d}\times\bbR^{d \times d} \mapsto \bbR^{d \times d}$ such that for a given step size $h>0$, we get
% \begin{align*}
%   \label{eq:matode}
%   \BF_{h}(\Psibf^{h}\VY,\VY) = 0 .
% \end{align*}
% \begin{hint}
% Check:
% \begin{align*}
%   \BF_{h}(\VX,\VY)=\VX-\VY+\frac{h}{4}(\VX +\VY-\VX^\top-\VY^\top)(\VX+\VY).
% \end{align*}
% \end{hint}
% 
% \begin{solution}
% The mid-point rule applied to \eqref{eq:diffeq} is
% \begin{align*}
% \VY_{k+1} &= \VY_k+hf\left(\frac{\VY_k+\VY_{k+1}}{2}\right)\\
% &=\VY_k-\frac{h}{4}(\VY_k+\VY_{k+1}-(\VY_k+\VY_{k+1}-)^\top)(\VY_k+\VY_{k+1}).
% \end{align*}
% It follows that
% \[
% 	\VY_{k+1}-\VY_{k}+\frac{h}{4}(\VY_k+\VY_{k+1}-(\VY_k+\VY_{k+1}-)^\top)(\VY_k+\VY_{k+1}) =0,
% \]
% So we define
% \[
% 	\BF_{h}(\VX,\VY):=\VX-\VY+\frac{h}{4}(\VX +\VY-\VX^\top-\VY^\top)(\VX+\VY).
% \]
% \end{solution}
% \end{subproblem}
% 
% \begin{subproblem}[1]\label{sp:newton} Write a Newton iteration ($\to$ Numerische Mathematik I), solving the system \eqref{eq:matode} for $Y_{1} := \Psibf^{h}\VY$, explicitly,
% by first finding the partial derivative of $\BF_{h}$ by the first matrix argument.
% 
% \begin{hint}
% The derivative of a general function $\BG:\bbR^{d \times d} \mapsto \bbR^{d \times d}$ is a function $D\BG:\bbR^{d\times d}\mapsto L(\bbR^{d \times d},\bbR^{d \times d})$, where $L(\bbR^{d \times d},\bbR^{d \times d})$ is the space of linear functions $\bbR^{d \times d} \mapsto \bbR^{d \times d}$.  These sorts of derivatives are best written down as $D\BG(\VX)(\VH)$ for $\VX,\VH\in\bbR^{d\times d}$. 
% The calculation uses either the general product and chain rules or repeated Taylor expansions for the expression
% \begin{align*}
%   \BG(\VX+\VH)-\BG(\VX) = D\BG(\VX)(\VH) + O(\N{\VH}^{2}).
% \end{align*}
% \end{hint}
% 
% \begin{solution}
% Observe the function
% \[
% 	\BF_h(\BX,\VY_k)=\VX-\VY_k+\frac{h}{4}(\VX +\VY_k-\VX^\top-\VY_k^\top)(\VX+\VY_k).
% \]
% For a matrix $\delta\VX$ with $\norm{\delta\VX}<1$ we get
% \begin{align*}
% 	\VF_h(\VX+\delta\VX,\VY_k) &=\VX+\delta\VX-\VY_k+\frac{h}{4}(\VX +\delta\VX+\VY_k-\VX^\top-\delta\VX^\top-\VY_k^\top)(\VX+\delta\VX+\VY_k)\\
% 	&=\VX-\VY_k+\delta\VX+\frac{h}{4}\big((\VX +\VY_k-\VX^\top-\VY_k^\top)+(\delta\VX-\delta\VX^\top)\big)\big((\VX+\VY_k)+\delta\VX\big)\\
% 	&=\VX-\VY_k+\delta\VX+\frac{h}{4}\big((\VX +\VY_k-\VX^\top-\VY_k^\top)(\VX+\VY_k) + \ldots \\
% 	&\phantom{{}={}}\ldots + (\VX +\VY_k-\VX^\top-\VY_k^\top)\delta\VX +(\delta\VX-\delta\VX^\top)(\VX+\VY_k)+(\delta\VX-\delta\VX^\top)\delta\VX\big)\\
% 	&=\VF_h(\VX,\VY_k) +\big(\VI +\frac{h}{4}(\VX +\VY_k-\VX^\top-\VY_k^\top)\big)\delta\VX \\
% 	&\phantom{{}={}}\ldots + \frac{h}{4}(\delta\VX -\delta\VX^\top)(\VX +\VY_k)+O(\delta\VX^2)
% \end{align*}
% From this we conclude that
% \[
% 	\VD_{\VX}\VF_h(\VX,\VY_k)\delta \VX=\big(\VI +\frac{h}{4}(\VX +\VY_k-\VX^\top-\VY_k^\top)\big) \delta\VX +\frac{h}{4}(\delta\VX -\delta\VX^\top)(\VX +\VY_k).
% \]
% The Newton iteration is then given by
% \[
% 	\VX^{(k+1)}=\VX^{(k)}-\delta\VX,
% \]
% where $\delta\VX$ satisfies the matrix equation
% \begin{equation}\label{eq:eqdeltaX}
% 	\VD_{\VX}\VF_h(\VX^{(k)},\VY_k)\delta\VX=\VF_h(\VX^{(k)},\VY_k).
% \end{equation}
% \end{solution}
% \end{subproblem}
% 
% \begin{subproblem}[1]\label{sp:matrices}
% The Newton iteration for solving $\BF_{h}(\VY_{1},\VY_{0})= 0$ for $\VY_{1}\in\bbR^{d \times d}$, as developed in the previous subproblem, requires the solving of a matrix equation
% \begin{align*}\label{eq:mateq}
% 	\VA\VX+\VX\VB+\VX^\top\VC=\VD,
% \end{align*}
% with unknown matrix $\VX \in \bbR^{d \times d}$ in every step. Find the matrices $\VA$, $\VB$ and $\VC$
% in terms of $\VY_{0}$ and $h$. 
% 
% \begin{solution}
%  
% We can read the four matrices directly from \eqref{eq:eqdeltaX} ($\VX^{(k)}$ is the root calculated by $k$ Newton iterations, we choose $\VX^{(0)}=\VY_k$ as initial value)
% \begin{align*}
% 	\VA&=\big(\VI +\frac{h}{4}(\VX^{(k)} +\VY_k-(\VX^{(k)})^\top-\VY_k^\top)\big)\\
% 	\VB&=\frac{h}{4}(\VX^{(k)}+\VY_k)\\
% 	\VC&=-\frac{h}{4}(\VX^{(k)}+\VY_k)\\
% 	\VD&=\VF_h(\VX^{(k)},\VY_k).
% \end{align*}
% \end{solution}
% \end{subproblem}
% 
% \begin{subproblem}[1] \label{sp:mateqsolve}
% Implement a \matlab function
% \begin{center}
%   \texttt{function X = mateqsolve(A,B,C,D)},
% \end{center}
% that solves the matrix equation \eqref{eq:mateq} with the \matlab $\backslash$ operator ,
% where the $d \times d$ matrices $\VA$, $\VB$, $\VC$ and $\VD$ are given as arguments \texttt{A},
% \texttt{B}, \texttt{C} and \texttt{D} respectively.
% 
% \hint{In principle, the $\backslash$ operator in \matlab can solve equations of the form $\VA\Vx=\Vb$.
% So our linear matrix equations must be written as ``matrix $\times$ vector=vector''. This happens by transforming into vectors the matrices with the \texttt{reshape} function in \matlab, by writing the columns one above the other, f.e. for a $d \times d$ matrix \texttt{v = reshape(V,d*d,1)}. To construct the $d^{2} \times d^{2}$ matrix for the corresponding  system of equations, the \texttt{kron} function in matrix offers itself.
% First, think about how the $d^{2} \times d^{2}$ matrix $\VM=\VM(\VA)$ must look, so that the procedure
% \begin{quote}
%   \texttt{x=reshape(X,d*d,1); y = M*x; Y = reshape(y,d,d);}
% \end{quote}
% creates a matrix $\VY\in\bbR^{d \times d}$, for which
% (i) $\VY = \VA\VX$, (ii) $\VY=\VX\VA$, (iii) $\VY=\VX^{T}\VA$, with a given matrix $\VA$ respectively.} \\
% 
% \begin{solution}
% We have
% \begin{enumerate}[(i)]
% \item \texttt{reshape(A*X,d*d,1)=kron(eye(n),A)*reshape(X,d*d,1)},
% \item \texttt{reshape(X*B,d*d,1)=kron(B',eye(n))*reshape(X,d*d,1)},
% \item \texttt{reshape(X'*C,d*d,1)=kron(C',eye(n))*P*reshape(X,d*d,1)},
% \end{enumerate}
% where the permutation matrix \texttt{P} satisfies
% \begin{center}
% \texttt{P*reshape(X,d*d,1)=reshape(X',d*d,1).}
% \end{center}
% For example, for $d=2$, $\texttt{P}=\begin{pmatrix}1&0&0&0\\0&0&1&0\\0&1&0&0\\0&0&0&1\end{pmatrix},$
% while for $d=3$
% \[
% 	\texttt{P}=\begin{pmatrix}
% 	1&0&0&0&0&0&0&0&0\\
% 	0&0&0&1&0&0&0&0&0\\
% 	0&0&0&0&0&0&1&0&0\\
% 	0&1&0&0&0&0&0&0&0\\
% 	0&0&0&0&1&0&0&0&0\\
% 	0&0&0&0&0&0&0&1&0\\
% 	0&0&1&0&0&0&0&0&0\\
% 	0&0&0&0&0&1&0&0&0\\
% 	0&0&0&0&0&0&0&0&1\\
% 	\end{pmatrix}
% \]
% % The implementation is included in code \prbref{code:mateqsolve}
% % \lstinputlisting[
% % emph={mateqsolve},label={code:mateqsolve},caption={Solving Matrix Equations}]{matlab/mateqsolve.m}
%  \end{solution}
% \end{subproblem}
% 
% % 
% % \begin{subproblem}[1] \label{sp:ortho}
% % Prove that, for $h>0$ \emph{sufficiently} small, $\BPsi^{h}$ defines a self-mapping on the group of orthogonal matrices.
% % 
% % \begin{hint}
% % We also have to show that the equation \eqref{eq:matode} can be solved for $\BPsi^{h}\VY$ when $h>0$ is sufficiently small. Here, the Theorem about implicit functions can be used. You may quote \ncseref[Lemma]{lem:midp}.
% % \end{hint}
% % \begin{solution}
% %  First off, we show that \eqref{eq:matode} can be solved for $\BPsi^{h}$ if $h>0$ is small enough.
% % We define a function $\VG:\bbR\times\bbR^{d \times d}\to \bbR^{d \times d}$ by
% % \[
% % 	\VG(t,\VX):=\VF_t(\VX,\VY),
% % \]
% % where $\VY\in\bbR^{d\times d}$ is a chosen constant matrix. Then:
% % \begin{itemize}
% % 	\item $\VG(0,\VY)=0$,
% % 	\item $\VG(t,\VX)$ is continuously differentiable by $t$ and $\VX$ (polynomials are continuously differentiable),
% % 	\item $\VD_{\VX}\VG(t,\VX) = I + O(t)$, so $\VD_{\VX}\VG(0,\VX)$ is definitely invertible.
% % \end{itemize}
% % Thanks to the theorem about implicit functions, there exists a neighbourhood $U$ of $0$ and a continuously differentiable function $\xi \mapsto \Vz(\xi)$, such that $\VG(\xi,\Vz(\xi))=0$ for all $\xi$ in $U$. Then we have $$\BPsi^{h}=\Vz(h).$$
% % Now, we show that $\BPsi^{h}$ is a self-mapping on the group of orthogonal matrices. From \autoref{prb:matrix-diff-eq} we know that $I(\VY):=\VY^\top\VY$ is a first integral of \eqref{eq:diffeq}. Each entry of $I(\VY):=(i_{mn})_{m,n=1}^d$ is quadratic, as
% % \[
% % 	i_{xy}=\texttt{reshape}(\VY,d^2,1)^\top \cdot \VA \cdot \texttt{reshape}(\VY,d^2,1)
% % \]
% % for a block matrix $\VA \in \bbR^{d^2 \times d^2}$, $\VA=(\VA_{mn})_{m,n=1}^d$ with
% % \[
% % 	\VA_{mn}=\begin{cases}\VI &\text{if } m=x,n=y\\ 0 &\text{else}\end{cases}
% % \]
% % By \ncseref{lem:midp} we know further that the semi-implicit mid-point rule preserves quadratic first integrals, so $I(\BPsi^{h}\VY_0)=I(\VY_0)$. If $I(\VY_0)=\VI$, then also $I(\BPsi^{h}\VY_0)=\VI$, so $\BPsi^{h}\VY_0$ is an orthogonal matrix.
% %  \end{solution}
% % \end{subproblem}
% 
% \begin{subproblem}[1] \label{sp:matodesmp}
% Write a \matlab function
% \begin{center}
%   \texttt{function Y=matodesmp(T,Y0,N,n)}
% \end{center}
% which takes an end point \texttt{T}, an initial value \texttt{Y0} and a number \texttt{N} of steps of the same size, and returns an approximation of the solution of \eqref{eq:diffeq} with the implicit mid-point rule by \texttt{n} Newton iterations.
% In each step, use the approximation from the previous step as initial value for the Newton method.\\
% % \solution{See \prbautoref{code:matodesmp}
% % \lstinputlisting[
% % emph={matodesmp},label={code:matodesmp},caption={Semi-implicit mid-point rule}]{matlab/matodesmp.m}
% % }
%  \begin{solution}
%   
%  \end{solution}
% \end{subproblem}
% 
% 
% \begin{subproblem}[1] \label{sp:convspmp}
% As in \autoref{prb:matrix-diff-eq:sp:convrate}, write a \matlab function
% \begin{center}
%   \texttt{function convsmp()}
% \end{center}
% which finds the convergence rate of the semi-implicit mid-point rule in a numerical experiment. Further, create a suitable plot,
% which displays the error vs. the number of integration steps.
%  
%  \begin{solution}
%  See the implementation in \autoref{code:convspmp}. For the figure see \autoref{fig:convspmp}.
% %  \lstinputlisting[
% % emph={convsmp},label={code:convspmp},caption={Convergence of the semi-implicit mid-point rule}]{matlab/convsmp.m}
% % \begin{figure}[htb]
% % \centering
% % \includegraphics[width=0.5\textwidth]{fig/convsmp.eps}
% % \caption{Convergence plot of the semi-implicit mid-point rule}
% % \label{fig:convspmp}
% % \end{figure}
%  \end{solution}
% 
% \end{subproblem}

\end{problem}